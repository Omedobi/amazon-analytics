{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "#Load the necessary libraries.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os, sys\n",
    "import itertools\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "pd.set_option('Display.max_rows', None)\n",
    "pd.set_option('Display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective = Sales forecast using time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../amazon.csv')\n",
    "\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Wrangling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[['rating','rating_count']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Data_cleaning script\n",
    "\n",
    "sys.path.append('../src')\n",
    "from Data_cleaning import DataHandling\n",
    "\n",
    "cleaner = DataHandling(df1)\n",
    "\n",
    "cleaner.apply_cleaning()\n",
    "\n",
    "df_cleaned = cleaner.get_cleaned_dataframe()\n",
    "\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle missing values using SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "Imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "\n",
    "df1[['Rating','Rating_count']] = Imputer.fit_transform(df1[['Rating','Rating_count']])\n",
    "\n",
    "df1[['Rating','Rating_count']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "clean category column, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "df1.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "new_column_name = {\n",
    "    'Discounted_price':'Discounted_price_$',\n",
    "    'Actual_price':'Actual_price_$',\n",
    "    'Discount_percentage':'Discount_percentage_%',\n",
    "}\n",
    "\n",
    "df1.rename(columns=new_column_name, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning categorical columns\n",
    "\n",
    "df1.select_dtypes(include='object').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes(exclude='object')\n",
    "cat_attributes = df1.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Attributes\n",
    "describe = num_attributes.describe().T\n",
    "\n",
    "describe['range'] = (num_attributes.max() - num_attributes.min()).tolist()\n",
    "describe['variation coefficient'] = (num_attributes.std() / num_attributes.mean()).tolist()\n",
    "describe['skew'] = num_attributes.skew().tolist()\n",
    "describe['kurtosis'] = num_attributes.kurtosis().tolist()\n",
    "\n",
    "describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical aatributes\n",
    "\n",
    "cat_attributes.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an only categorical and numerical features dataframe\n",
    "num_df = df1.select_dtypes(include='float')\n",
    "cat_df = df1.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning text or categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical dataframe\n",
    "cat_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category = '|', user_id to review_title = ',' # clean accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "from Data_cleaning import CategoricalHandling\n",
    "\n",
    "cat_cleaner = CategoricalHandling(cat_df)\n",
    "\n",
    "df2_cleaned = cat_cleaner.apply_cat(lambda x: x.lower() if isinstance(x, str) else x, columns=['User_name'])\n",
    "\n",
    "df2_cleaned[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# splits and flatten a column based on the delimiter , or |\n",
    "def split_and_flatten(column):\n",
    "    split_values = column.str.split('[|,]', regex=True).to_list()\n",
    "    flat_list = list(itertools.chain.from_iterable(split_values))\n",
    "    return flat_list\n",
    "\n",
    "#Pad the list to the same size/shape\n",
    "def pad_list(lists):\n",
    "    max_len = max(len(lst) for lst in lists)\n",
    "    return [lst + [None] * (max_len - len(lst)) for lst in lists] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global dict to store processed data\n",
    "flattened_data = {}\n",
    "#take account of maximum length encountered\n",
    "max_len = 0\n",
    "\n",
    "for column in df2_cleaned.columns:\n",
    "    flattened_column = split_and_flatten(df2_cleaned[column])\n",
    "    flattened_data[column] = flattened_column\n",
    "    max_len = max(max_len, len(flattened_column))\n",
    "    \n",
    "for key in flattened_data:\n",
    "    while len(flattened_data[key]) < max_len:\n",
    "        flattened_data[key].append(None)\n",
    "    \n",
    "flattened_data = pd.DataFrame(flattened_data)\n",
    "\n",
    "for column in flattened_data.columns:\n",
    "    if flattened_data[column].dtype == object:\n",
    "        flattened_data[column].fillna('Missing', inplace=True)\n",
    "    else:\n",
    "        flattened_data[column].fillna(flattened_data[column].mode()[0], inplace=True)\n",
    "        \n",
    "        \n",
    "flattened_data = flattened_data[~flattened_data.isin(['Missing']).any(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "print(flattened_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge them together to create one dataframe by creating a new index.\n",
    "num_df.reset_index(inplace=True)\n",
    "\n",
    "flattened_data.reset_index(inplace=True)\n",
    "\n",
    "df2 = pd.merge(num_df,flattened_data, left_on='index', right_on='index', how='outer')\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the index column and save the dataframe \n",
    "if 'index' in df2.columns:\n",
    "    df2.drop(columns='index', inplace=True)\n",
    "    \n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df2.to_csv('../data/amazon.csv', index=False)\n",
    "print('New Dataframe saved as a csv file...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive Statistical analysis on df2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dataset by their features\n",
    "\n",
    "num_attributes = df2.select_dtypes(exclude='object')\n",
    "cat_attributes = df2.select_dtypes(include='object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe = num_attributes.describe().T\n",
    "\n",
    "describe['range'] = (num_attributes.max() - num_attributes.min()).tolist()\n",
    "describe['variation coefficient'] = (num_attributes.std() / num_attributes.mean()).tolist()\n",
    "describe['skew'] = num_attributes.skew().tolist()\n",
    "describe['kurtosis'] = num_attributes.kurtosis().tolist()\n",
    "\n",
    "\n",
    "describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sale analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue calculation\n",
    "\n",
    "#calculate the revenue for each product\n",
    "df2[\"Revenue_$\"] = df2['Discount_percentage_%'] * df2['Rating_count']\n",
    "\n",
    "#calculate the revenue for all products\n",
    "total_revenue = df2['Revenue_$'].sum()\n",
    "\n",
    "print(f'Total Revenue: ${total_revenue}')\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Product: Best selling product by revenue\n",
    "top_prod_sorted = df2.sort_values(by='Revenue_$', ascending=False)\n",
    "\n",
    "Top_product = top_prod_sorted\n",
    "\n",
    "print(f\"Top Best-Selling Product by Revenue:\\n {Top_product[['Product_id', 'Product_name', 'Revenue_$']][:6]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Category: Which Product categories generate the most revenue\n",
    "\n",
    "#group the category by the sum of revenue\n",
    "category_revenue = df2.groupby('Category')[['Product_name','Revenue_$']].sum().reset_index()\n",
    "#sort the category \n",
    "category_revenue_sorted = category_revenue.sort_values(by='Revenue_$', ascending=False)\n",
    "#identify the top categories\n",
    "top_categories = category_revenue_sorted\n",
    "\n",
    "print(f\"Top Product categories by Revenue: \\n {top_categories.head(5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# create a possible date for purchase date\n",
    "\n",
    "start_date = datetime(2021,1,1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "\n",
    "def random_date(start, end):\n",
    "    \"\"\"Generate random datetime between start and end.\"\"\"\n",
    "    return start + timedelta(days=np.random.randint(0, (end - start).days))\n",
    "\n",
    "df2['Purchase_date'] = df2.apply(lambda x: random_date(start_date, end_date), axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer segmentation based on their purchase behaviour\n",
    "\n",
    "#confirm the datetime preparation\n",
    "today = datetime.now()\n",
    "df2['Purchase_date'] = pd.to_datetime(df2['Purchase_date'])\n",
    "\n",
    "#Calculate Recency, Frequency, and Monetary values for each customer\n",
    "rfm = df2.groupby('User_id').agg({\n",
    "    'Purchase_date': lambda x: (today - x.max()).days,\n",
    "    'User_id': 'count',\n",
    "    'Discounted_price_$': 'sum'\n",
    "})\n",
    "\n",
    "#rename columns\n",
    "rfm.columns = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "rfm = rfm.reset_index()\n",
    "\n",
    "#rank customers based on RFM values\n",
    "rfm['R_rank'] = rfm['Recency'].rank(ascending=True)\n",
    "rfm['F_rank'] = rfm['Frequency'].rank(ascending=False)\n",
    "rfm['M_rank'] = rfm['Monetary'].rank(ascending=False)\n",
    "\n",
    "#create a combined RFM score by summing the ranks\n",
    "rfm['RFM_Score'] = rfm['R_rank'] + rfm['F_rank'] + rfm['M_rank']\n",
    "\n",
    "#Segment customers based on the RFM score\n",
    "rfm['Customer_Segment'] = pd.qcut(rfm['RFM_Score'].astype(int), q=4, labels=['Low', 'Medium','High', 'VIP'])\n",
    "\n",
    "rfm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.to_csv('../data/cleaned/rfm.csv', index=False)\n",
    "\n",
    "print(rfm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Price Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "\n",
    "#calculating additional metric\n",
    "\n",
    "df2['Discount_amount'] = df2['Actual_price_$'] - df2['Discounted_price_$']\n",
    "df2['Sales_volume'] = df2['Rating_count'] #using this as proxy for sales volumne\n",
    "\n",
    "#categorize discount into bins\n",
    "discount_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "discount_labels = ['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%']\n",
    "df2['Discount_bin'] = pd.cut(df2['Discount_percentage_%'], bins=discount_bins, labels=discount_labels, include_lowest=True)\n",
    "\n",
    "#Group the discount bin and calculate average sales volumne\n",
    "discount_effectiveness = df2.groupby('Discount_bin').agg({\n",
    "    'Sales_volume':['mean','sum'],\n",
    "    'Discount_percentage_%':'count'\n",
    "}).reset_index()\n",
    "\n",
    "#Rename the columns\n",
    "discount_effectiveness.columns = ['Discount_bin', 'Avg_Sales_Volume', 'Total_Sales_Volume', 'Count']\n",
    "\n",
    "# Statistical test to compare sales volumes across discount levels\n",
    "no_discount_sales = df2[df2['Discount_percentage_%'] == 0]['Sales_volume']\n",
    "discounted_sales = df2[df2['Discount_percentage_%'] > 0]['Sales_volume']\n",
    "\n",
    "# T-test for two independent samples\n",
    "t_stat, p_val = ttest_ind(no_discount_sales, discounted_sales, equal_var=False)\n",
    "print(f\"T-test results: t-statistic = {t_stat}, p-value = {p_val}\")\n",
    "\n",
    "# comparing more than two groups (ANOVA)\n",
    "anova_results = f_oneway(*[df2[df2['Discount_bin'] == bin]['Sales_volume'] for bin in discount_labels if bin in df2['Discount_bin'].values])\n",
    "print(f\"ANOVA results: F-statistic = {anova_results.statistic}, p-value = {anova_results.pvalue}\")\n",
    "\n",
    "discount_effectiveness.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Price Elasticity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate percentage change in price and quantity demanded\n",
    "df2['Price_change_%'] = df2['Actual_price_$'].pct_change() * 100\n",
    "df2['Quantity_change_%'] = df2['Sales_volume'].pct_change() * 100\n",
    "\n",
    "#Handle NaN values \n",
    "df2[['Price_change_%','Quantity_change_%']] = df2[['Price_change_%','Quantity_change_%']].fillna(df2[['Price_change_%','Quantity_change_%']].mean())\n",
    "\n",
    "#Calculate Price Elasticity of Demand (PED)\n",
    "df2['PED'] = df2['Quantity_change_%'] / df2['Price_change_%']\n",
    "\n",
    "print(df2[['Product_id', 'Actual_price_$', 'Sales_volume', 'Price_change_%', 'Quantity_change_%', 'PED']][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the NaN in PED with the median\n",
    "df2['PED'] = df2['PED'].fillna(df2['PED'].median())\n",
    "#check for NaN values\n",
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the selling price\n",
    "\n",
    "new_column = {\n",
    "    \"Discount_amount\":\"Selling_price_$\"\n",
    "}\n",
    "\n",
    "df2.rename(columns=new_column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('../data/cleaned/amazon.csv', index=False)\n",
    "print(f'Successfully saved the new dataframe in a csv file format')\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Univariate analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bivariate analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested Deep Learning and Traditional models\n",
    "1.   ARIMA – Captures time-dependent patterns and trends.\n",
    "2.   LSTM – Uses deep learning to capture long-term dependencies in sequential data.\n",
    "3. Decision Tree Regressor – A baseline traditional model that handles non-sequential data.\n",
    "4. Random Forest Regressor – Improves on decision trees through ensembling.\n",
    "5. SVM (Support Vector Machine) – A sensitive model that can handle non-linear relationships, good for small datasets.\n",
    "6. ANN (Artificial Neural Network) – A powerful but computationally intensive deep learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert 'Purchase_date' to datetime and sort the dataset by it\n",
    "df2['Purchase_date'] = pd.to_datetime(df2['Purchase_date'])\n",
    "df2 = df2.sort_values('Purchase_date')  # Corrected the typo\n",
    "df2.set_index('Purchase_date', inplace=True)\n",
    "\n",
    "# 2. Create time series data for ARIMA\n",
    "time_series_data = df2['Revenue_$']\n",
    "\n",
    "# 3. Check stationarity using ADF test\n",
    "\n",
    "\n",
    "adf_result = adfuller(time_series_data)\n",
    "print(f\"ADF Statistic: {adf_result[0]}\")\n",
    "print(f\"p-value: {adf_result[1]}\")\n",
    "\n",
    "if adf_result[1] > 0.05:\n",
    "    print(f\"Time series is non-stationary, Differencing needed\")\n",
    "else:\n",
    "    print(f\"Time series is stationary, no differencing needed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "# 1. Define and fit ARIMA model on 'time_series_data' (p=1, d=0, q=1 as a starting point; adjust as needed)\n",
    "arima_model = ARIMA(time_series_data, order=(1, 0, 1))  # Adjust order as needed\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "# 2. Make predictions on the test set\n",
    "# Define start and end for test set predictions\n",
    "train_size = int(len(time_series_data) * 0.4)  # 40% training, 60% testing\n",
    "start = train_size  # Start of test set\n",
    "end = len(time_series_data) - 1  # End of dataset\n",
    "\n",
    "# Generate predictions for the test period\n",
    "arima_preds = arima_result.predict(start=start, end=end)\n",
    "\n",
    "# Separate the actual test values for evaluation\n",
    "y_test_arima = time_series_data[start:]\n",
    "\n",
    "# 3. Evaluate the ARIMA model\n",
    "arima_mse = mean_squared_error(y_test_arima, arima_preds)\n",
    "arima_mae = mean_absolute_error(y_test_arima, arima_preds)\n",
    "arima_r2 = r2_score(y_test_arima, arima_preds)\n",
    "\n",
    "print(f'ARIMA - MSE: {arima_mse}, MAE: {arima_mae}, R2: {arima_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add time-based features\n",
    "df2['month'] = df2.index.month\n",
    "df2['day_of_week'] = df2.index.dayofweek\n",
    "df2['quarter'] = df2.index.quarter\n",
    "\n",
    "# Add lagged and rolling average features\n",
    "df2['Revenue_Lag1'] = df2['Revenue_$'].shift(1)\n",
    "df2['Revenue_Lag2'] = df2['Revenue_$'].shift(2)\n",
    "df2['Revenue_7d_avg'] = df2['Revenue_$'].rolling(window=7).mean()\n",
    "df2['Revenue_30d_avg'] = df2['Revenue_$'].rolling(window=30).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with NaN values created by rolling/lagging\n",
    "df2 = df2.dropna()\n",
    "\n",
    "# Separate features and target\n",
    "features = df2.drop(columns=['Revenue_$'])\n",
    "target = df2['Revenue_$']\n",
    "\n",
    "# Ensure only numeric features for model input or encode the catergorically variables\n",
    "features = features.select_dtypes(include=[float, int])\n",
    "\n",
    "#replace infinity values with NaN, then fill with mean\n",
    "features = features.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "#check for infinity values\n",
    "print(\"Infinity values:\", np.isinf(features).sum())\n",
    "\n",
    "#check for very large values\n",
    "print(\"max values in each column:\")\n",
    "print(features.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets (40% train, 60% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.6, shuffle=False)\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for LSTM (samples, timesteps, features)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, activation='relu', input_shape=(1, X_train_scaled.shape[1])))\n",
    "lstm_model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "# Compile and train the model\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "lstm_preds = lstm_model.predict(X_test_lstm)\n",
    "lstm_preds_rescaled = lstm_preds.flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Evaluation\n",
    "lstm_mse = mean_squared_error(y_test, lstm_preds_rescaled)\n",
    "lstm_mae = mean_absolute_error(y_test, lstm_preds_rescaled)\n",
    "lstm_r2 = r2_score(y_test, lstm_preds_rescaled)\n",
    "\n",
    "print(f'LSTM - MSE: {lstm_mse}, MAE: {lstm_mae}, R2: {lstm_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "dt_preds = dt_model.predict(X_test_scaled)\n",
    "\n",
    "dt_mse = mean_squared_error(y_test, dt_preds)\n",
    "dt_mae = mean_absolute_error(y_test, dt_preds)\n",
    "dt_r2 = r2_score(y_test, dt_preds)\n",
    "\n",
    "print(f\"Decision Tree - MSE: {dt_mse}, MAE: {dt_mae}, R2: {dt_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_preds = rf_model.predict(X_test_scaled)\n",
    "\n",
    "rf_mse = mean_squared_error(y_test, rf_preds)\n",
    "rf_mae = mean_absolute_error(y_test, rf_preds)\n",
    "rf_r2 = r2_score(y_test, rf_preds)\n",
    "\n",
    "print(f\"Random Forest - MSE: {rf_mse}, MAE: {rf_mae}, R2: {rf_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "svm_model = SVR()\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_preds = svm_model.predict(X_test_scaled)\n",
    "\n",
    "svm_mse = mean_squared_error(y_test, svm_preds)\n",
    "svm_mae = mean_absolute_error(y_test, svm_preds)\n",
    "svm_r2 = r2_score(y_test, svm_preds)\n",
    "\n",
    "print(f\"SVM - MSE: {svm_mse}, MAE: {svm_mae}, R2: {svm_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network (ANN)\n",
    "\n",
    "\n",
    "ann_model = Sequential()\n",
    "ann_model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "ann_model.add(Dense(32, activation='relu'))\n",
    "ann_model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='mse')\n",
    "ann_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "ann_preds = ann_model.predict(X_test_scaled)\n",
    "ann_preds_rescaled = ann_preds.flatten()\n",
    "\n",
    "ann_mse = mean_squared_error(y_test, ann_preds_rescaled)\n",
    "ann_mae = mean_absolute_error(y_test, ann_preds_rescaled)\n",
    "ann_r2 = r2_score(y_test, ann_preds_rescaled)\n",
    "\n",
    "print(f\"ANN - MSE: {ann_mse}, MAE: {ann_mae}, R2: {ann_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best fit models: Decision Tree and \n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n",
    "Random Forest\n",
    "\n",
    "---\n",
    "Reason: R2 closest to value of 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
